# Comentarios semana 5: Combinando recomendadores

**Paper:** Jahrer, M., Töscher, A. and Legenstein, R. (2010). Combining predictions for accurate recommender systems. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 693-702. ACM.

En este trabajo se demuestra empíricamente (data set de Netflix) que la performance (RMSE) de una combinación adecuada de modelos es superior a la marginal de cada uno de ellos. Los métodos usados son los conocidos KNN item; KNN user; SVD y tres variaciones: AFM (asymmetric factor model), el cual se enfoca en caracterizar a los usuarios por medio de sus ítems consumidos. SVDe (SVD extended), añade característica de frecuencia de votos y GE (global effects), una factorización matricial con un diseño de características hechas a mano. Además se condidera un modelo generativo (es decir, que busca aprender la distribución de probabilidad de los datos) con neuronas estocásticas y optimizando la consistencia ("energía"), llamado RBM (restricted bolzmann machine). 

El trabajo motiva los modelos de mezcla como un problema de aprendizaje supervisado pero donde el ajuste al target se realiza a través de una función que combina los recomendadores antes descritos de alguna forma, por ejemplo un promedio. Lo cual es cierto, sin embargo yo lo motivaría de una manera distinta:

Desde un punto de vista ingenuo podría parecer confuso que el promedio de un grupo de predictores (con mayor error) puedan disminuir el error de predicción, pero hay que tener en mente el dilema sesgo-varianza, el cual *identifica* el error de predicción como una suma de la varianza y sesgo del modelo ( y un término que depende del ruido). Así, asumiendo modelos *iid* a través de un cálculo sencilo se puede notar que el sesgo de un promedio sigue siendo el sesgo de cada uno de ellos (al sacar escalar), pero por otro lado, la varianza del promedio resulta ser la varianza de un modelo dividido por la cantidad de ellos. En resumen, al tomar un promedio de modelos *iid* estamos manteniendo el sesgo, por lo que al menos mantenemos el error de aproximación al "mejor modelo". Pero también estamos reduciendo monótamente la varianza, y por tanto al considerar más modelos disminuimos el error de estimación. Luego, como consecuencia de la *indentificación* al promediar se obtiene un predictor más preciso y confiable. De esa manera la intuición es que **la unión hace la fuerza** bajo condiciones de independecia. Que si bien es algo idílico, en la práctica existen técnicas que a través de remuestreos con reemplazo logran emular en algún grado la independiencia.      

Los modelos de mezclas utilizados en el trabajo fueron: LR (linear regression), el cual combina los métodos a través de pesos óptimos vía minimos cuadrados. Binned Linear Regression, para evitar muestras muy grandes realiza mezclas sobre particiones. NN (neuronal network) constituido por una optimización SGD, activación sigmoide y una transformación de salida para escalar los ratings. BGBDT (bagged gradient boosted decision tree), este combina una ténica de bagging (búsqueda de divisiones óptimas) con gradient boosting (todos los predictores aportan aprendiendo una proporción del objetivo) y con selección aleatoria de subespacios óptimos. KRR (kernel ridge regression blending), promedia modelos que invierten la matriz Grammiana con un kernel Gaussiano. KNN, que es un promedio de una combinación por similaridad de modelos entrenados sobre un subconjunto de entrenamiento. 

Finalmente, los autores muestran como aplicando el principio de sesgo-varianza explicado anteriormente sobre todos los modelos de mezclas, se obtiene el mejor error de predicción, pero a un costo computacional en tiempo muy alto. Debido a esto, los autores recomiendan para una rápida velocidad de predicción, utilizar la mezcla NN, osea, entrenar una red para aprender el target real dado los vectores de predicciones hechos por los recomendadores elementales.





