# Comentarios semana 5: Combinando recomendadores

**Paper:** Jahrer, M., Töscher, A. and Legenstein, R. (2010). Combining predictions for accurate recommender systems. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 693-702. ACM.

En este trabajo se demuestra empíricamente (data set de Netflix) que la performance (RMSE) de una combinación adecuada de modelos es superior a la marginal de cada uno de ellos. Los métodos usados son los conocidos KNN item; KNN user; SVD y tres variaciones: AFM (asymmetric factor model), se enfoca en caracterizar a los usuarios por medio de sus ítems consumidos. SVDe (SVD extended), añade característica de frecuencia de votos y GE (global effects), una factorización matricial con un diseño de característica hecho a mano. Además se condidera un modelo generativo (busca aprender la distribución de probabilidad de los datos) con neuronas estocásticas y optimizando la consistencia ("energía"), llamado RBM (restricted bolzmann machine). 

El trabajo motiva los modelos de mezcla como un problema de aprendizaje supervisado pero donde el ajuste al target se realiza através de una función que combina los recomendadores antes descritos de alguna forma, por ejemplo un promedio. Lo cual es cierto, sin embargo yo lo motivaría de una manera distinta: Desde un punto de vista ingenuo podría parecer confuso que el promedio de un grupo de predictores (con mayor error) puedan disminuir el error de predicción pero hay que tener en mente el dilema sesgo-varianza el cual identifica el error de predicción como una suma de la varianza y sesgo del modelo ( y un término que depende del ruido). Así asumiendo modelos *iid* a través de un cálculo sencilo podemos ver que el sesgo de un promedio sigue siendo el sesgo de cada uno de ellos (al ser sacar escalar) pero por otro lado la varianza del promedio resulta ser la varianza de un modelo divido por la cantidad de ellos que se están considerado. En resumen, al tomar un promedio de modelos *iid* estamos manteniendo el sesgo por lo que al menos mantenemos el error de aproximación al "mejore modelo", pero también estamos reduciendo monótamente la varianza y por tanto al considerar más modelos disminuimos el error de estimación, teniendo como consecuencia de la indentificación un predictor más preciso y confiable. De esa manera la intuición es que **la unión hace la fuerza** bajo condiciones de independecia. Que si bien es algo idílico, en la práctica existen técnicas que a través de remuestreos con reemplazo logran emular en algún grado la independiencia.      

Los modelos de mezclas utilizados en el trabajo fueron: LR (linear regression), el cual combina los métodos a través de pesos óptimos vía minimos cuadrados. Binned Linear Regression, para evitar muestras muy grandes realiza mezclas sobre particiones. NN (neuronal network) con SGD, sigmoide y una transformación para entregar escalar los ratings. BGBDT (bagged gradient boosted decision tree), combina una ténica de bagging (búsqueda de divisiones óptimas), gradient boosting (todos los predictores aportan aprendiendo una proporción del objetivo) y selección aleatoria de subespacios óptimos. KRR (kernel ridge regression blending), promedia modelos que invierten la matriz Grammiana con un kernel Gaussiano. KNN, promedio de una combinación por similaridad de modelos entrenados sobre un subconjunto de entrenamiento. 

En conclusión, los autores muestran como aplicando el principio explicado anteriormente (bagging) sobre todos los modelos de mezclas, se obtiene el mejor error pero a un costo computacional en tiempo muy alto. Por ello recomiendan utilizar la mezcla NN (osea, entrenar una red para aprender el target real dado los vectores de predicciones hechos por los recomendadores) junto a bagging para una rápida velocidad de predicción.    





