# Comentarios semana 4: Sistemas de recomendación basados en contenido

**Paper:** Pazzani, M. J., & Billsus, D. (2007). Content-based recommendation systems. In The adaptive web (pp. 325-341). Springer Berlin Heidelberg. Xu, W., Liu, X., & Gong, Y. (2003)

Este paper trata acerca de los modelos de recomencación basados en la descripción de los ítems y/o la construcción de un perfil del usuario.

Una forma usual de data para una consulta *q* es el texto, la cual no está sujeta a una estructura fija y trae toda la complejidad del entendimiento del lenguaje natural. Usualmante esta data no estucturada se transforma en una estructurada, formada por valores reales que representan la importancia o la relevancia de ciertos términos *t* que conforman el texto, estos valores forma una representación *tf.idf* (term-frequency yimes invers document frequency) caracterizada por pesos que se calculan para cada *t* en un documento *d* como una función de que considera los siguiente aspectos: la frecuencia de *t* en *d*, el número de documentos que contienen a *t* y el número de documentos totales *N*. La intuición detrás de la fórmula de pesos es que los términos con mayor valor son los que aparecen más usualmente en *d* respecto a los demás documentos y por tanto son los tópicos más centrales del documento. Notar que esta representación no es capaz de capturar el contexto en el cual el término o palabra es usada, y tiene el problema entonces, de darle relevancia a palabras que se presentan en un documento pero no son relevantes en el contexto. También yo añadiría que es relavante el pre procesamiento del texto para evitar palabras usuales que no aportan relevancia al mensaje, las llamadas *stop words* o también los carácteres alphanumericos.

Un tema escencial para construir sistemas de recomendación basados en contenido es la descripción de perfiles de preferencias de los usuarios y la construcción de estos modelos se pueden basar en dos principales principios:  predecir/aprender para todos los ítems la verosimilitud del interés que podría mostrar el usuario, o bien almacenar las interacciones del usuario con el sistema para construir reglas de asociación. Común mente esto se puede lograr a través de detección de patrones o aprendiendo un modelo usualmente de clasificación, los cuales usualmente utilizan feedback implícito pues es más fácil recolectar datos pero en desmedro de la confiabilidad de que los datos estén representando correctamente las preferencias de los usuarios. Uno de los algoritmos clásicos de clasificación son los árboles y dentro de ellos, destacar la regla inductiva llamada *RIPPER* el cual se destaca por una sofisticada post poda del árbol, lo que permite aliviar el usual sesgo a preferir regiones demasiadas específicas en las cuales es poco probable que existan usuarios de test y por tanto debilita la generalización. También es una opción *knn* con similaridad coseno, cuando queremos generar clusters de tópicos y para ello es suficiente tener una de las características cercanas en el sentido del ángulo de la representación. 

Existen métodos que se basan en la relvancia del feedback, y su principio es permitir a los usuarios calificar los documentos entregados por el sistema con respecto a la información que ellos necesitaban. El algoritmo de *Rocchio* se basa en la modificación pesada de una consulta inicial através de ciertos términos. Primero de una suma pesada de documentos relevantes, lo cual incrementa el movimiento del vector de consulta hacia el grupo de documentos relevante. Y luego, de la resta pesada de documentos no relevantes que aleja el vector de consulta de este grupo inapropiado. Este método a mostrado una mejora en la performance de los sistemas de recuperación de información pero no cuenta con un fundamento teórico que asegure la convergencia.

Otra posibilidad son los clasificadores lineales, estos buscan generar hiperplanos que logren clasificar los datos introduciendo técnicas que permitan mejorar la generalización y cuidarse del overfitting, como lo hace *SVM*.

También pueden aplicarse enfoques probabilístico, principalmente el modelo generativo *Naive Bayes* que se basa en suposiciones de independencia para las clases, con el objetivo de contar con un cálculo óptimo de la verosimilítud para luego explotar la regla de Bayes. Este presenta una buena performance y es aconsejable para la clasificación de texto. 

En conclusión, me parece que el paper es demasiado general y se enfoca en mencionar posibles modelos para consultas de búsqueda de documentos, desde el punto de vista de la clasificación. Pero no logra reunir metodologías innovadoras para las tareas de recomendación que se basen más allá de vectorizar (con un enfoque frecuentista) el texto.  
