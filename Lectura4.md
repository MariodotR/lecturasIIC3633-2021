# Comentarios semana 4: Sistemas de recomendación basados en contenido

**Paper:** Pazzani, M. J., & Billsus, D. (2007). Content-based recommendation systems. In The adaptive web (pp. 325-341). Springer Berlin Heidelberg. Xu, W., Liu, X., & Gong, Y. (2003)

Es paper trata acerca de los modelos de recomencación basados en la descripción de los ítems y el perfil del usuario de interés. Un forma usual de data es el texto la cual no está sujeta a una estructura fija y trae toda la complejidad del entendimiento del lenguaje natural. Usualmante esta data no estuctura se transforma en una estructurada, formada por valores reales que representan la importancia o la relevancia aquellos conforman una representación *tf.idf* caracterizada por pesos que se calculan para cada térmnino *t* en un documento *d* como una función de la frecuencia de *t* en el mismo, del número de documentos que contienen un término *t* y el número de documentos *N*. La intuición detrás de la fórmula de pesos es que los términos con mayor valor son los que aparecen más usualmente en ese documento *d* respecto a los demás documentos y por tanto son los tópicos más centrales del documento. Notar que esta representación no es capaz de capturar el contexto en el cual el término o palabra es usada y tiene el problema entonces de darle relevancia a palabras que se presentan en un documento pero no son relevantes en el contexto. También yo añadiría que es relavante el pre procesamiento del texto para evitar palabras usuales que no aportan relevancia al mensaje, las llamadas *stop words* o carácteres alphanumericos.

Descripción de perfiles de preferencias de los usuarios, la construcción de modelos se basa en dos principales principios, para todo items predecir la verosimilitud del interes que podria mostrar el usuarios, o almacenar las interacciones del usurio con el sistema. Esto se puede lograr a través de reglas de asociación o aprendiendo un modelo usualmente de clasificación estos usualmente utilizan feedback implicito pues es más fácil recolectar datos en desmedro de la confiabilidad de que los datos estén reprentando correctamente las preferencias de los usuarios. Uno de los algoritmos clásicos declasificación son los arboles, dentro de ellos destacar la regla inductiva llamada *RIPPER* el cual se destaca por una sofisticada post poda del arbol lo que permite aliviar el usual sesgo a preferir regiones demasiadas especificas en las cuales es poco probable que existan usuarios de test y va en contra de la generalización. También para el caso de *knn* con similaridad coseno es una opción cuando queremos generar clusters de topicos, en donde sea suficiente tener una de las características cercanas. 

Un metodo que se basa en la relvancia del feedback su principio es permitir a los usuarios calificar los documentos entregados por el sistema con respecto a la información que el necesitaba. El algoritmo de *Rocchio* se basa en la modificación de una consulta inicial através de una suma pesada de documentos relevantes, incrementando el movimiento del vector de consulta hacia el grupo de documentos relevantes y la resta pesada de documentos no relevantes que aleja el vector de consulta de este grupo. Este metodo a mostrado una mejora en la performance de los sistemas de recuperación de información pero no cuenta con un fundamento teorico que segure la convergencia.

Los clasificadores lineales son aquellos que buscan generar hiperplanos que logren clasificar los datos introduciendo tecnicas que permitan mejorar la generalización y cuidarse del overfitting como *SVM*.

Métodos probabilistico principalmente el modelo generativo *Naive Bayes* que se basa en suposiciones de independencia para las clases para un calculo optimo de la verosimilitud para luego explotar la regla de Bayes, este presenta una buena performance para clasificación de texto 
