# Comentarios semana 2: Técnicas de factorización matricial para sistemas recomendadores.

**Paper:** Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, 42(8), 30-37.

El presente paper presenta técnicas de filtrado colaborativo (CF) basado en técnicas de factorización matricial, en particular el método *SVD*. Estas fueron introducidas durante la competencia del *Netflix Price*. Los modelos de factorización matricial surgen a partir de las ideas que establecen los modelos latentes, los cuales buscan transformar los datos con el objetivo de construir características o dimensiones (ocultas o latentes) que permitan entender los patrones de la data original. Algunas ventajas de estos moodelos son: que significan una mejora general de accuracy con respecto a los modelos (CF) basados en KNN, son más flexibles para incorporar información adicional como *implicit feedback*, y además, permiten interpretar clusters en la nueva dimensionalidad de ítems-usuarios. 

La principal crítica que les hago a los autores es que introducen el método como uno cercano a *SVD* ( del área de recuperación de información) pero no describen por qué, ni que características o intuición comparten. Para enmendar aquello, realice una estudio más profundo de *SVD* basado principalmente en el post original de [FunkSVD](https://sifter.org/~simon/journal/20061211.html). Así yo establecería que **la motivación del método es elaborar una buena extracción de características**. La explicación se basa en asumir que la calificación que un usuario entrega a un ítem se compone de diferentes preferencias sobre ciertos aspectos específicos del ítem. Basado en esto, es que cobra sentido la incorporación del método *SVD*, al **considerar el razonamiento anterior como uno bidireccional**. Estos es, que si suponemos que existen aquellos aspectos generales y significativos que pueden ayudar a decribir los ítems con menor dimensionalidad, entonces, encontrar una manera de representar los ítems en un espacio de menor dimensionalidad pudiese ayudar a encontrar estos aspectos significativos (feautures). Resumidamente el método *SVD* permite reconstruir matrices a través de aproximaciones de rango bajo para los mayores o más importantes valores singulares, luego por el razonamiento anterior está técnica es un extractor de características.

Un factor distintivo al método clásico es que la matriz usuario-ítems es altamente sparse por lo que imputar es altamente costoso y se debe repensar el cálculo de la factorización matricial. Los autores platean el problema como uno de aprendizaje estadístico, así, el objetivo es encontrar las nuevas representaciones de los usurios e ítems tales que minimizan la distancia entre los ratings disponibles y el producto punto de estas representaciones. Además como el riesgo a overfitting es alto al ser muy sparse la matriz de ratings es que se añade una regularización *l2*. Notar entonces que se entrenará el mododelo para que aproxime los ratings a través de la suma de preferencias de ciertos aspectos generales entre el ítem y el usuario, intentando así aprender a capturar la relación entre usuarios e ítems. Luego el proceso de recomendación se basa en predecir el rating al multiplicar los vectores de características para el par usuario-ítem según corresponda. Dado el modelamiento del problema es claro el problema de *cold-start* pues si no hay ratings disponibles no se puede localizar de manera precisa el par usuario e ítem en el espacio de características aprendido. 

Para resolver el problema de optimización los autores proponen dos métodos: el popular avance en contra de la dirección del gradiente, stochastic gradient descent *SGD* y un método basado en resolver problemas de minimos cuadrados aprovechando que al fijar una de las dos argumentos de la función objetivo, el problema se transforma en uno de optimización cuadrática, llamado alternating least squares *ALS*. Acertadamente los autores comparan los métodos, que resumo en el siguiente cuadro:

| SGD  | ALS             | 
|:--------|:-----------------|
| Más fácil de interpretar      | Paralelizable, los cálculos de representaciones no depende del resto de usuarios-ítems      |
| Más rápido      | Más eficiente con data implicita (no sparse), pues no itera sobre cada caso del train     |

Destacar que los autores refinan el problema de optimización incorporándole a la interacción usuario-ítem, el promodio global y los sesgos de ítem y usuario. También proponen la incorporación del tiempo sobre los usuarios con el objetivo de capturar la evolución de los gustos, lo cual entrega los mejores resultados de accuracy. Además, incorporan el nivel de confianza de cada rating através de una matriz en el problema de optimización. 

Para concluir me gustaría destacar que los autores presentan en sus resultados que todos los modelos mejoran en accuracy al incrementar el numéro de dimensiones, y dado la intuición del método, yo afirmaría que el numéro de factores o aspectos latentes a considerar es el hiperparámetro más importante a sintonizar con la data particular de trabajo.
